{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:25:45.080958Z",
     "iopub.status.busy": "2025-05-18T11:25:45.080750Z",
     "iopub.status.idle": "2025-05-18T11:25:46.117848Z",
     "shell.execute_reply": "2025-05-18T11:25:46.117196Z",
     "shell.execute_reply.started": "2025-05-18T11:25:45.080941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df for question answer file csv contains 1 Lakh data points\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/kaggle/input/qna-1l/cleaned_qna.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:25:46.118916Z",
     "iopub.status.busy": "2025-05-18T11:25:46.118639Z",
     "iopub.status.idle": "2025-05-18T11:25:46.879564Z",
     "shell.execute_reply": "2025-05-18T11:25:46.878738Z",
     "shell.execute_reply.started": "2025-05-18T11:25:46.118893Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# to read images.csv from vr-miniproject-2 or one can upload it manually and then copy the relative path\n",
    "file_path = pd.read_csv(\"/kaggle/input/vr-miniproject-2/abo-images-small/images/metadata/images.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:25:46.986078Z",
     "iopub.status.busy": "2025-05-18T11:25:46.985832Z",
     "iopub.status.idle": "2025-05-18T11:25:47.207763Z",
     "shell.execute_reply": "2025-05-18T11:25:47.206600Z",
     "shell.execute_reply.started": "2025-05-18T11:25:46.986054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# merging qna with images\n",
    "final_df = pd.merge(df, file_path, on='image_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:21:38.076222Z",
     "iopub.status.busy": "2025-05-18T11:21:38.075920Z",
     "iopub.status.idle": "2025-05-18T11:21:38.086875Z",
     "shell.execute_reply": "2025-05-18T11:21:38.086123Z",
     "shell.execute_reply.started": "2025-05-18T11:21:38.076189Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01dkn0Gyx0L</td>\n",
       "      <td>What is the glass's color?</td>\n",
       "      <td>Clear</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>da/daab0cad.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01dkn0Gyx0L</td>\n",
       "      <td>What is the overall form?</td>\n",
       "      <td>Cylindrical</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>da/daab0cad.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01dkn0Gyx0L</td>\n",
       "      <td>What is the material?</td>\n",
       "      <td>Glass</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>da/daab0cad.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01dkn0Gyx0L</td>\n",
       "      <td>What is the glass finish?</td>\n",
       "      <td>Smooth</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>da/daab0cad.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01dkn0Gyx0L</td>\n",
       "      <td>What is the glass's appearance?</td>\n",
       "      <td>Transparent</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>da/daab0cad.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                         question       answer  height  width  \\\n",
       "0  01dkn0Gyx0L       What is the glass's color?        Clear     122    122   \n",
       "1  01dkn0Gyx0L        What is the overall form?  Cylindrical     122    122   \n",
       "2  01dkn0Gyx0L            What is the material?        Glass     122    122   \n",
       "3  01dkn0Gyx0L        What is the glass finish?       Smooth     122    122   \n",
       "4  01dkn0Gyx0L  What is the glass's appearance?  Transparent     122    122   \n",
       "\n",
       "              path  \n",
       "0  da/daab0cad.jpg  \n",
       "1  da/daab0cad.jpg  \n",
       "2  da/daab0cad.jpg  \n",
       "3  da/daab0cad.jpg  \n",
       "4  da/daab0cad.jpg  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:25:47.209586Z",
     "iopub.status.busy": "2025-05-18T11:25:47.209250Z",
     "iopub.status.idle": "2025-05-18T11:25:47.216906Z",
     "shell.execute_reply": "2025-05-18T11:25:47.216046Z",
     "shell.execute_reply.started": "2025-05-18T11:25:47.209556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# to select specific col\n",
    "final_df = final_df[[\"image_id\",\"path\",\"question\",\"answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:25:47.218088Z",
     "iopub.status.busy": "2025-05-18T11:25:47.217826Z",
     "iopub.status.idle": "2025-05-18T11:25:47.277014Z",
     "shell.execute_reply": "2025-05-18T11:25:47.276294Z",
     "shell.execute_reply.started": "2025-05-18T11:25:47.218066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# adjusting the path of images according to kaggle\n",
    "import os\n",
    "image_folder = '/kaggle/input/vr-miniproject-2/abo-images-small/images/small/'\n",
    "final_df['path'] = final_df['path'].apply(lambda p: os.path.join(image_folder, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T10:30:31.443846Z",
     "iopub.status.busy": "2025-05-18T10:30:31.443563Z",
     "iopub.status.idle": "2025-05-18T10:30:31.458785Z",
     "shell.execute_reply": "2025-05-18T10:30:31.457947Z",
     "shell.execute_reply.started": "2025-05-18T10:30:31.443826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28299 entries, 0 to 28298\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image_id  28299 non-null  object\n",
      " 1   path      28299 non-null  object\n",
      " 2   question  28299 non-null  object\n",
      " 3   answer    28223 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 884.5+ KB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T10:30:35.172678Z",
     "iopub.status.busy": "2025-05-18T10:30:35.172120Z",
     "iopub.status.idle": "2025-05-18T10:30:35.180149Z",
     "shell.execute_reply": "2025-05-18T10:30:35.179450Z",
     "shell.execute_reply.started": "2025-05-18T10:30:35.172654Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>path</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>What color is the keyboard?</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>What shape is the cable?</td>\n",
       "      <td>Circular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>What is the item?</td>\n",
       "      <td>Keyboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>What material is visible?</td>\n",
       "      <td>Plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>Where are the letters?</td>\n",
       "      <td>Keys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                               path  \\\n",
       "0  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "1  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "2  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "3  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "4  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "\n",
       "                      question    answer  \n",
       "0  What color is the keyboard?     Black  \n",
       "1     What shape is the cable?  Circular  \n",
       "2            What is the item?  Keyboard  \n",
       "3    What material is visible?   Plastic  \n",
       "4       Where are the letters?      Keys  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:25:47.281514Z",
     "iopub.status.busy": "2025-05-18T11:25:47.281263Z",
     "iopub.status.idle": "2025-05-18T11:27:04.095016Z",
     "shell.execute_reply": "2025-05-18T11:27:04.094276Z",
     "shell.execute_reply.started": "2025-05-18T11:25:47.281492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.31.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.4.26)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-score\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "Successfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "# install bert score package\n",
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T13:22:21.116Z",
     "iopub.execute_input": "2025-05-18T11:27:04.097291Z",
     "iopub.status.busy": "2025-05-18T11:27:04.096721Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'BARTScore'...\n",
      "remote: Enumerating objects: 220, done.\u001b[K\n",
      "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 220 (delta 18), reused 14 (delta 14), pack-reused 194 (from 1)\u001b[K\n",
      "Receiving objects: 100% (220/220), 101.98 MiB | 23.01 MiB/s, done.\n",
      "Resolving deltas: 100% (47/47), done.\n"
     ]
    }
   ],
   "source": [
    "# install bart score\n",
    "!git clone https://github.com/neulab/BARTScore.git\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/BARTScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-05-18T11:27:25.677490Z",
     "shell.execute_reply": "2025-05-18T11:27:25.676795Z",
     "shell.execute_reply.started": "2025-05-18T11:27:11.589895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# requiered classes to do fine tuning\n",
    "!pip install -q git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:27:25.679472Z",
     "iopub.status.busy": "2025-05-18T11:27:25.679250Z",
     "iopub.status.idle": "2025-05-18T11:27:25.696855Z",
     "shell.execute_reply": "2025-05-18T11:27:25.696138Z",
     "shell.execute_reply.started": "2025-05-18T11:27:25.679453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# change question and answert to lower case\n",
    "final_df[\"question\"] = final_df[\"question\"].str.lower()\n",
    "final_df[\"answer\"] = final_df[\"answer\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:27:25.697819Z",
     "iopub.status.busy": "2025-05-18T11:27:25.697647Z",
     "iopub.status.idle": "2025-05-18T11:27:25.715020Z",
     "shell.execute_reply": "2025-05-18T11:27:25.714310Z",
     "shell.execute_reply.started": "2025-05-18T11:27:25.697805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# just to run some sample to test if the model is working or not, if one is sure one can work just remove .iloc[0:1000]\n",
    "df = final_df.iloc[0:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:21:55.048101Z",
     "iopub.status.busy": "2025-05-18T11:21:55.047840Z",
     "iopub.status.idle": "2025-05-18T11:21:55.066407Z",
     "shell.execute_reply": "2025-05-18T11:21:55.065642Z",
     "shell.execute_reply.started": "2025-05-18T11:21:55.048084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image_id  1000 non-null   object\n",
      " 1   path      1000 non-null   object\n",
      " 2   question  1000 non-null   object\n",
      " 3   answer    1000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    " df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:27:25.716426Z",
     "iopub.status.busy": "2025-05-18T11:27:25.715908Z",
     "iopub.status.idle": "2025-05-18T11:27:25.728363Z",
     "shell.execute_reply": "2025-05-18T11:27:25.727617Z",
     "shell.execute_reply.started": "2025-05-18T11:27:25.716380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/821518018.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"question\"] = df[\"question\"].str.lower()\n",
      "/tmp/ipykernel_35/821518018.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"answer\"] = df[\"answer\"].str.lower()\n"
     ]
    }
   ],
   "source": [
    "# change question and answert to lower case\n",
    "df[\"question\"] = df[\"question\"].str.lower()\n",
    "df[\"answer\"] = df[\"answer\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:27:25.729346Z",
     "iopub.status.busy": "2025-05-18T11:27:25.729123Z",
     "iopub.status.idle": "2025-05-18T11:27:25.755776Z",
     "shell.execute_reply": "2025-05-18T11:27:25.755119Z",
     "shell.execute_reply.started": "2025-05-18T11:27:25.729326Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>path</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>what color is the keyboard?</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>what shape is the cable?</td>\n",
       "      <td>circular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>what is the item?</td>\n",
       "      <td>keyboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>what material is visible?</td>\n",
       "      <td>plastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81K3c1sX-XL</td>\n",
       "      <td>/kaggle/input/vr-miniproject-2/abo-images-smal...</td>\n",
       "      <td>where are the letters?</td>\n",
       "      <td>keys</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                               path  \\\n",
       "0  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "1  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "2  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "3  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "4  81K3c1sX-XL  /kaggle/input/vr-miniproject-2/abo-images-smal...   \n",
       "\n",
       "                      question    answer  \n",
       "0  what color is the keyboard?     black  \n",
       "1     what shape is the cable?  circular  \n",
       "2            what is the item?  keyboard  \n",
       "3    what material is visible?   plastic  \n",
       "4       where are the letters?      keys  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T18:16:33.822867Z",
     "iopub.status.busy": "2025-05-13T18:16:33.822558Z",
     "iopub.status.idle": "2025-05-13T18:19:11.702207Z",
     "shell.execute_reply": "2025-05-13T18:19:11.701271Z",
     "shell.execute_reply.started": "2025-05-13T18:16:33.822841Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:16:51.425025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747160211.901102      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747160212.040057      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb86cdf4c48f44208a85bbe59a372003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0761c89eb4e341cfb9ebdb36884c0838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7d898274d8457681d6e4763201e8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37a854fc3cf4fda9f8fa07544ceb0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27858a46b4484610bef51b1a23e71a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5367da827744df812fc1f4e25e0578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5582c920ca24c89b3eba34fd6781664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ba751a2a834d57905355d5060d905d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180952b9939b4e018017a73334999012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d05aeea2ea4477ad537cf5998fee8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1610bfc9664f480c9adf1a4bf428ffee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c55853b192453d845fa877f3804160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/5.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d592cd25c3454eb78354c7b2db994e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6f48c669084d299d484455a919f866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this was done to train initially\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from peft import PeftModelForCausalLM, LoraConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Subclass Blip2ForConditionalGeneration to modify the forward method\n",
    "class CustomBlip2ForConditionalGeneration(Blip2ForConditionalGeneration):\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values = None,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,  # LoRA may add inputs_embeds\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "        start_positions = None,\n",
    "        end_positions = None,\n",
    "    ):\n",
    "        if inputs_embeds is not None:\n",
    "            print(\"Warning: inputs_embeds are being ignored. Using input_ids instead.\")\n",
    "            inputs_embeds = None  # Ignore inputs_embeds\n",
    "        if start_positions is not None:\n",
    "            print(\"Warning: inputs_embeds are being ignored. Using input_ids instead.\")\n",
    "            start_positions = None\n",
    "        if end_positions is not None:\n",
    "            print(\"Warning: inputs_embeds are being ignored. Using input_ids instead.\")\n",
    "            end_positions = None\n",
    "        # Continue with BLIP-2's original forward behavior\n",
    "        return super().forward(\n",
    "            pixel_values = pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            use_cache = use_cache,\n",
    "        )\n",
    "\n",
    "# to define model and load it\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model = CustomBlip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\",\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quant_config,\n",
    ")\n",
    "\n",
    "# to create dataset for vqa for fine tuning\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, max_target_length=4):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "        #image = image_transform(image)\n",
    "        question = row[\"question\"]\n",
    "        answer = row[\"answer\"]\n",
    "\n",
    "        # Use proper prompt template\n",
    "        prompt = f\"Answer in one word only: {question}\"\n",
    "\n",
    "        encoding = self.processor(\n",
    "            image,\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=50,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize the answer with padding and truncation\n",
    "        target = self.processor.tokenizer(\n",
    "            answer,\n",
    "            padding=\"max_length\",  # Ensure padding is added to max_length\n",
    "            truncation=True,\n",
    "            max_length=self.max_target_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        encoding[\"labels\"] = target.input_ids.squeeze(0)\n",
    "\n",
    "        return {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "# collate function\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        key: torch.stack([item[key] for item in batch])\n",
    "        for key in batch[0]\n",
    "    }\n",
    "\n",
    "# Data split\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_dataset = VQADataset(train_df, processor)\n",
    "val_dataset = VQADataset(val_df, processor)\n",
    "\n",
    "# one can change batch size inorder to run it faster (if it's very large then leads to out of memory issue)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T20:30:46.365241Z",
     "iopub.status.busy": "2025-05-17T20:30:46.365045Z",
     "iopub.status.idle": "2025-05-17T20:34:09.824365Z",
     "shell.execute_reply": "2025-05-17T20:34:09.823522Z",
     "shell.execute_reply.started": "2025-05-17T20:30:46.365217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 20:30:56.418495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747513856.625646      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747513856.688909      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62b8237fdf84ab9a222a6b2471748c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36052e89440c454eb7aacf0b7496ba54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885abb91facf4f25a633bf48579af605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb690f9e4064bd780a84a813617010c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b2dbff0ff242b4aee356b54cc5725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a1d8407aeb4e18b78d3aba2204f077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c102683392459489cbb7ac6d09a2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2f42117d114c9ca4837a63c1924feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063e8441ac4242d9bb740d91264b0c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec31b496c881405993a9ba44629fc597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2472034d41424ebd529485417c6810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/5.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab28587b02a44567a485fae97fe7280e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b352ec48ce414dee8d242937ab0b9d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a487ce80cced442184016445319a28ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# in this we did for training model after training it for some datapoints \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from peft import PeftModelForCausalLM, LoraConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Subclass Blip2ForConditionalGeneration to modify the forward method\n",
    "class CustomBlip2ForConditionalGeneration(Blip2ForConditionalGeneration):\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values = None,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,  # LoRA may add inputs_embeds\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "        start_positions = None,\n",
    "        end_positions = None,\n",
    "    ):\n",
    "        if inputs_embeds is not None:\n",
    "            print(\"Warning: inputs_embeds are being ignored. Using input_ids instead.\")\n",
    "            inputs_embeds = None  # Ignore inputs_embeds\n",
    "        if start_positions is not None:\n",
    "            print(\"Warning: inputs_embeds are being ignored. Using input_ids instead.\")\n",
    "            start_positions = None\n",
    "        if end_positions is not None:\n",
    "            print(\"Warning: inputs_embeds are being ignored. Using input_ids instead.\")\n",
    "            end_positions = None\n",
    "        # Continue with BLIP-2's original forward behavior\n",
    "        return super().forward(\n",
    "            pixel_values = pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            use_cache = use_cache,\n",
    "        )\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "# Reload everything properly\n",
    "base_model = CustomBlip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\",\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "# Now load the PEFT model\n",
    "model = PeftModel.from_pretrained(base_model, '/kaggle/input/blip-finetune3')\n",
    "\n",
    "\n",
    "\n",
    "# to create dataset for vqa for fine tuning\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, max_target_length=4):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "        #image = image_transform(image)\n",
    "        question = row[\"question\"]\n",
    "        answer = row[\"answer\"]\n",
    "\n",
    "        # Use proper prompt template\n",
    "        prompt = f\"Answer in one word only: {question}\"\n",
    "\n",
    "        encoding = self.processor(\n",
    "            image,\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=50,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize the answer with padding and truncation\n",
    "        target = self.processor.tokenizer(\n",
    "            answer,\n",
    "            padding=\"max_length\",  # Ensure padding is added to max_length\n",
    "            truncation=True,\n",
    "            max_length=self.max_target_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Remove unnecessary padding tokens from labels\n",
    "        encoding[\"labels\"] = target.input_ids.squeeze(0)\n",
    "\n",
    "        # Make sure everything is properly squeezed and returned as a dictionary\n",
    "        return {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        key: torch.stack([item[key] for item in batch])\n",
    "        for key in batch[0]\n",
    "    }\n",
    "\n",
    "# Data split\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_dataset = VQADataset(train_df, processor)\n",
    "val_dataset = VQADataset(val_df, processor)\n",
    "\n",
    "# one can change batch size inorder to run it faster (if it's very large then leads to out of memory issue)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T15:30:19.688974Z",
     "iopub.status.busy": "2025-05-10T15:30:19.688363Z",
     "iopub.status.idle": "2025-05-10T15:30:19.698619Z",
     "shell.execute_reply": "2025-05-10T15:30:19.697877Z",
     "shell.execute_reply.started": "2025-05-10T15:30:19.688950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this is used to find module names which can be used for fine tuning as it should be all linear\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T17:52:45.012311Z",
     "iopub.status.busy": "2025-05-11T17:52:45.011926Z",
     "iopub.status.idle": "2025-05-11T17:52:45.016432Z",
     "shell.execute_reply": "2025-05-11T17:52:45.015773Z",
     "shell.execute_reply.started": "2025-05-11T17:52:45.012288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# to observe different tasks available in peft to train \n",
    "from peft import TaskType\n",
    "print([t for t in TaskType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T18:19:11.703819Z",
     "iopub.status.busy": "2025-05-13T18:19:11.703190Z",
     "iopub.status.idle": "2025-05-13T18:19:14.442521Z",
     "shell.execute_reply": "2025-05-13T18:19:14.441780Z",
     "shell.execute_reply.started": "2025-05-13T18:19:11.703785Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 150,994,944 || all params: 4,093,441,536 || trainable%: 3.6887\n"
     ]
    }
   ],
   "source": [
    "# PEFT: LoRA configuration  (in this made change to fine tune specific to question and answer)\n",
    "from peft import LoraConfig, TaskType\n",
    "# one can change here r, alpha, dropout, bias, target_modules (for target_modules select from above layers if u want additional modules)\n",
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q\", \"v\", \"k\",\"o\"],\n",
    "    task_type=TaskType.QUESTION_ANS\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T18:20:04.328316Z",
     "iopub.status.busy": "2025-05-13T18:20:04.327648Z",
     "iopub.status.idle": "2025-05-14T03:47:53.534926Z",
     "shell.execute_reply": "2025-05-14T03:47:53.534247Z",
     "shell.execute_reply.started": "2025-05-13T18:20:04.328292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pixel Stats] min: -1.777664065361023, max: 2.1458969116210938, mean: 1.6644349098205566\n",
      "\n",
      "[Prompt]      Answer in one word only: what is on the boxes?\n",
      "[Prediction]   label\n",
      "[GroundTruth] label\n",
      "\n",
      "[Prompt]      Answer in one word only: what are the accessories?\n",
      "[Prediction]  \n",
      "[GroundTruth] screws\n",
      "  Step 0: Loss = 2.8697\n",
      "  Step 1000: Loss = 1.1424\n",
      "  Step 2000: Loss = 1.3797\n",
      "  Step 3000: Loss = 1.1977\n",
      "Epoch 1 average loss: 1.5117\n",
      "Validation loss after epoch 1: 1.2724\n",
      "\n",
      "Epoch 2\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.2333284616470337\n",
      "\n",
      "[Prompt]      Answer in one word only: what shape is it?\n",
      "[Prediction]  rectangle rectangle\n",
      "[GroundTruth] rectangle\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the surface?\n",
      "[Prediction]  ckled\n",
      "[GroundTruth] speckled\n",
      "  Step 0: Loss = 1.2899\n",
      "  Step 1000: Loss = 0.7424\n",
      "  Step 2000: Loss = 1.6143\n",
      "  Step 3000: Loss = 1.7314\n",
      "Epoch 2 average loss: 1.3221\n",
      "Validation loss after epoch 2: 1.1985\n",
      "\n",
      "Epoch 3\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.3185837268829346\n",
      "\n",
      "[Prompt]      Answer in one word only: what brand is visible?\n",
      "[Prediction]  organic\n",
      "[GroundTruth] wholefoods\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the bag's color?\n",
      "[Prediction]  black black\n",
      "[GroundTruth] black\n",
      "  Step 0: Loss = 1.9873\n",
      "  Step 1000: Loss = 0.7445\n",
      "  Step 2000: Loss = 1.8108\n",
      "  Step 3000: Loss = 0.9177\n",
      "Epoch 3 average loss: 1.2550\n",
      "Validation loss after epoch 3: 1.1532\n",
      "\n",
      "Epoch 4\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.6459938287734985\n",
      "\n",
      "[Prompt]      Answer in one word only: what is on the container?\n",
      "[Prediction]  texting\n",
      "[GroundTruth] lettering\n",
      "\n",
      "[Prompt]      Answer in one word only: what secures valuables?\n",
      "[Prediction]  lock\n",
      "[GroundTruth] safe\n",
      "  Step 0: Loss = 1.2491\n",
      "  Step 1000: Loss = 1.0833\n",
      "  Step 2000: Loss = 0.4542\n",
      "  Step 3000: Loss = 1.3203\n",
      "Epoch 4 average loss: 1.2169\n",
      "Validation loss after epoch 4: 1.1272\n",
      "\n",
      "Epoch 5\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.7339192628860474\n",
      "\n",
      "[Prompt]      Answer in one word only: where is the handle?\n",
      "[Prediction]  side side\n",
      "[GroundTruth] side\n",
      "\n",
      "[Prompt]      Answer in one word only: what shape is plate?\n",
      "[Prediction]  round\n",
      "[GroundTruth] round\n",
      "  Step 0: Loss = 1.1081\n",
      "  Step 1000: Loss = 0.9356\n",
      "  Step 2000: Loss = 1.6304\n",
      "  Step 3000: Loss = 1.2791\n",
      "Epoch 5 average loss: 1.1848\n",
      "Validation loss after epoch 5: 1.1062\n"
     ]
    }
   ],
   "source": [
    "# was trained for 0 to  30k data points (initially)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)  # Slightly increased learning rate\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model = model.to(torch.float32)\n",
    "model.train()\n",
    "\n",
    "# can change number of iteration  here\n",
    "for epoch in range(5): \n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)  # defaults to float32\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Mask padding tokens in labels to avoid loss explosion\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        # Check for NaN/Inf in pixel values and labels before forward pass\n",
    "        if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n",
    "            print(\"[Error] NaN/Inf in pixel values\")\n",
    "            continue\n",
    "        if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "            print(\"[Error] NaN/Inf in labels\")\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            task_ids=None\n",
    "        )\n",
    "        if torch.isnan(outputs.loss).any() or torch.isinf(outputs.loss).any():\n",
    "            print(f\"[Error] NaN/Inf loss at step {step}\")\n",
    "            print(f\"Logits: {outputs.logits}\")\n",
    "            continue\n",
    "        # Check for NaN/Inf loss\n",
    "        \n",
    "        # Decode predictions occasionally\n",
    "        if step == 0:\n",
    "            print(f\"[Pixel Stats] min: {pixel_values.min()}, max: {pixel_values.max()}, mean: {pixel_values.mean()}\")\n",
    "\n",
    "            predicted_ids = outputs.logits.argmax(dim=-1)\n",
    "            predicted_texts = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            decoded_labels = labels.clone()\n",
    "            decoded_labels[decoded_labels == -100] = processor.tokenizer.pad_token_id\n",
    "            ground_truth_texts = processor.batch_decode(decoded_labels, skip_special_tokens=True)\n",
    "\n",
    "            input_prompts = processor.batch_decode(input_ids, skip_special_tokens=True)\n",
    "            for i in range(min(2, len(predicted_texts))):\n",
    "                print(f\"\\n[Prompt]      {input_prompts[i]}\")\n",
    "                print(f\"[Prediction]  {predicted_texts[i]}\")\n",
    "                print(f\"[GroundTruth] {ground_truth_texts[i]}\")\n",
    "\n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"[Warning] NaN/Inf loss at step {step} — skipping batch\")\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # Lower clipping norm to prevent gradients explosion\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"  Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} average loss: {avg_loss:.4f}\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            input_ids = val_batch[\"input_ids\"].to(device)\n",
    "            pixel_values = val_batch[\"pixel_values\"].to(device)\n",
    "            attention_mask = val_batch[\"attention_mask\"].to(device)\n",
    "            labels = val_batch[\"labels\"].to(device)\n",
    "\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "            if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n",
    "                print(\"[Validation Error] NaN/Inf in pixel values\")\n",
    "                continue\n",
    "            if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "                print(\"[Validation Error] NaN/Inf in labels\")\n",
    "                continue\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            if torch.isnan(outputs.loss).any() or torch.isinf(outputs.loss).any():\n",
    "                print(f\"[Validation Error] NaN/Inf loss at val_step {val_step}\")\n",
    "                continue\n",
    "\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation loss after epoch {epoch + 1}: {avg_val_loss:.4f}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T06:59:15.993563Z",
     "iopub.status.busy": "2025-05-14T06:59:15.992499Z",
     "iopub.status.idle": "2025-05-14T13:08:40.010808Z",
     "shell.execute_reply": "2025-05-14T13:08:40.009932Z",
     "shell.execute_reply.started": "2025-05-14T06:59:15.993535Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 150,994,944 || all params: 4,093,441,536 || trainable%: 3.6887\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.2216002941131592\n",
      "\n",
      "[Prompt]      Answer in one word only: what color is the hardware?\n",
      "[Prediction]  black\n",
      "[GroundTruth] silver\n",
      "\n",
      "[Prompt]      Answer in one word only: what is this object?\n",
      "[Prediction]  glassr\n",
      "[GroundTruth] tumbler\n",
      "  Step 0: Loss = 1.0836\n",
      "  Step 1000: Loss = 0.4446\n",
      "  Step 2000: Loss = 0.5286\n",
      "  Step 3000: Loss = 1.2085\n",
      "Epoch 1 average loss: 1.1997\n",
      "Validation loss after epoch 1: 1.0571\n",
      "\n",
      "Epoch 2\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.1881518363952637\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the top's closure?\n",
      "[Prediction]  wrap button\n",
      "[GroundTruth] wrap\n",
      "\n",
      "[Prompt]      Answer in one word only: what kind of product is it?\n",
      "[Prediction]   tape\n",
      "[GroundTruth] tape\n",
      "  Step 0: Loss = 2.2381\n",
      "  Step 1000: Loss = 0.9203\n",
      "  Step 2000: Loss = 1.2957\n",
      "  Step 3000: Loss = 0.6742\n",
      "Epoch 2 average loss: 1.1440\n",
      "Validation loss after epoch 2: 1.0345\n",
      "\n",
      "Epoch 3\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.2756973505020142\n",
      "\n",
      "[Prompt]      Answer in one word only: what room is suggested?\n",
      "[Prediction]  bedroom bedroom\n",
      "[GroundTruth] bedroom\n",
      "\n",
      "[Prompt]      Answer in one word only: what color is the massager?\n",
      "[Prediction]  white white\n",
      "[GroundTruth] gray\n",
      "  Step 0: Loss = 1.2467\n",
      "  Step 1000: Loss = 1.6747\n",
      "  Step 2000: Loss = 1.2734\n",
      "  Step 3000: Loss = 0.7808\n",
      "Epoch 3 average loss: 1.1169\n",
      "Validation loss after epoch 3: 1.0180\n"
     ]
    }
   ],
   "source": [
    "# was trained for 30k to 60k data points (2nd)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)  # Slightly increased learning rate\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Force enable all adapters\n",
    "model = model.to(device)\n",
    "model = model.to(torch.float32)\n",
    "model.train()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name.lower():\n",
    "        param.requires_grad = True\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# can change number of iteration  here\n",
    "for epoch in range(3): \n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)  # defaults to float32\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Mask padding tokens in labels to avoid loss explosion\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        # Check for NaN/Inf in pixel values and labels before forward pass\n",
    "        if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n",
    "            print(\"[Error] NaN/Inf in pixel values\")\n",
    "            continue\n",
    "        if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "            print(\"[Error] NaN/Inf in labels\")\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            task_ids=None\n",
    "        )\n",
    "        if torch.isnan(outputs.loss).any() or torch.isinf(outputs.loss).any():\n",
    "            print(f\"[Error] NaN/Inf loss at step {step}\")\n",
    "            print(f\"Logits: {outputs.logits}\")\n",
    "            continue\n",
    "        # Check for NaN/Inf loss\n",
    "        \n",
    "        # Decode predictions occasionally\n",
    "        if step == 0:\n",
    "            print(f\"[Pixel Stats] min: {pixel_values.min()}, max: {pixel_values.max()}, mean: {pixel_values.mean()}\")\n",
    "\n",
    "            predicted_ids = outputs.logits.argmax(dim=-1)\n",
    "            predicted_texts = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            decoded_labels = labels.clone()\n",
    "            decoded_labels[decoded_labels == -100] = processor.tokenizer.pad_token_id\n",
    "            ground_truth_texts = processor.batch_decode(decoded_labels, skip_special_tokens=True)\n",
    "\n",
    "            input_prompts = processor.batch_decode(input_ids, skip_special_tokens=True)\n",
    "            for i in range(min(2, len(predicted_texts))):\n",
    "                print(f\"\\n[Prompt]      {input_prompts[i]}\")\n",
    "                print(f\"[Prediction]  {predicted_texts[i]}\")\n",
    "                print(f\"[GroundTruth] {ground_truth_texts[i]}\")\n",
    "\n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"[Warning] NaN/Inf loss at step {step} — skipping batch\")\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # Lower clipping norm to prevent gradients explosion\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"  Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} average loss: {avg_loss:.4f}\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            input_ids = val_batch[\"input_ids\"].to(device)\n",
    "            pixel_values = val_batch[\"pixel_values\"].to(device)\n",
    "            attention_mask = val_batch[\"attention_mask\"].to(device)\n",
    "            labels = val_batch[\"labels\"].to(device)\n",
    "\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "            if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n",
    "                print(\"[Validation Error] NaN/Inf in pixel values\")\n",
    "                continue\n",
    "            if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "                print(\"[Validation Error] NaN/Inf in labels\")\n",
    "                continue\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            if torch.isnan(outputs.loss).any() or torch.isinf(outputs.loss).any():\n",
    "                print(f\"[Validation Error] NaN/Inf loss at val_step {val_step}\")\n",
    "                continue\n",
    "\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation loss after epoch {epoch + 1}: {avg_val_loss:.4f}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T20:34:09.825871Z",
     "iopub.status.busy": "2025-05-17T20:34:09.825294Z",
     "iopub.status.idle": "2025-05-18T07:03:22.230278Z",
     "shell.execute_reply": "2025-05-18T07:03:22.229520Z",
     "shell.execute_reply.started": "2025-05-17T20:34:09.825851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 150,994,944 || all params: 4,093,441,536 || trainable%: 3.6887\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.3229995965957642\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the shape?\n",
      "[Prediction]  rectangular \n",
      "[GroundTruth] cylindrical\n",
      "\n",
      "[Prompt]      Answer in one word only: what reflects in the water?\n",
      "[Prediction]  moon\n",
      "[GroundTruth] moonlight\n",
      "  Step 0: Loss = 1.0912\n",
      "  Step 1000: Loss = 0.5028\n",
      "  Step 2000: Loss = 1.0605\n",
      "  Step 3000: Loss = 0.6530\n",
      "Epoch 1 average loss: 1.0703\n",
      "Validation loss after epoch 1: 0.9564\n",
      "\n",
      "Epoch 2\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.1476290225982666\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the main color of the script?\n",
      "[Prediction]  gold gold\n",
      "[GroundTruth] gold\n",
      "\n",
      "[Prompt]      Answer in one word only: what feature is present?\n",
      "[Prediction]  camera camera\n",
      "[GroundTruth] camera\n",
      "  Step 0: Loss = 1.3098\n",
      "  Step 1000: Loss = 0.6670\n",
      "  Step 2000: Loss = 0.8055\n",
      "  Step 3000: Loss = 1.0969\n",
      "Epoch 2 average loss: 1.0357\n",
      "Validation loss after epoch 2: 0.9377\n",
      "\n",
      "Epoch 3\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.3438653945922852\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the background's color?\n",
      "[Prediction]  black black\n",
      "[GroundTruth] black\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the material?\n",
      "[Prediction]  rubber rubber\n",
      "[GroundTruth] rubber\n",
      "  Step 0: Loss = 1.2495\n",
      "  Step 1000: Loss = 1.0238\n",
      "  Step 2000: Loss = 1.2340\n",
      "  Step 3000: Loss = 1.3798\n",
      "Epoch 3 average loss: 1.0156\n",
      "Validation loss after epoch 3: 0.9345\n",
      "\n",
      "Epoch 4\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.1529420614242554\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the pad's texture?\n",
      "[Prediction]  smooth smooth\n",
      "[GroundTruth] soft\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the material?\n",
      "[Prediction]  plastic plastic\n",
      "[GroundTruth] plastic\n",
      "  Step 0: Loss = 1.0304\n",
      "  Step 1000: Loss = 0.8153\n",
      "  Step 2000: Loss = 0.4580\n",
      "  Step 3000: Loss = 0.5640\n",
      "Epoch 4 average loss: 0.9981\n",
      "Validation loss after epoch 4: 0.9192\n",
      "\n",
      "Epoch 5\n",
      "[Pixel Stats] min: -1.7922625541687012, max: 2.1458969116210938, mean: 1.1273224353790283\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the outsole color?\n",
      "[Prediction]  black black\n",
      "[GroundTruth] black\n",
      "\n",
      "[Prompt]      Answer in one word only: what is the phone case's texture?\n",
      "[Prediction]  smoothtextured\n",
      "[GroundTruth] brushed\n",
      "  Step 0: Loss = 1.4672\n",
      "  Step 1000: Loss = 1.0743\n",
      "  Step 2000: Loss = 0.9134\n",
      "  Step 3000: Loss = 1.0916\n",
      "Epoch 5 average loss: 0.9833\n",
      "Validation loss after epoch 5: 0.9096\n"
     ]
    }
   ],
   "source": [
    "# was trained for 60k to 90k data points (3rd iteration)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)  # Slightly increased learning rate\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Force enable all adapters\n",
    "model = model.to(device)\n",
    "model = model.to(torch.float32)\n",
    "model.train()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name.lower():\n",
    "        param.requires_grad = True\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# can change number of iteration  here\n",
    "for epoch in range(5): \n",
    "    print(f\"\\nEpoch {epoch + 1}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)  # defaults to float32\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Mask padding tokens in labels to avoid loss explosion\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        # Check for NaN/Inf in pixel values and labels before forward pass\n",
    "        if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n",
    "            print(\"[Error] NaN/Inf in pixel values\")\n",
    "            continue\n",
    "        if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "            print(\"[Error] NaN/Inf in labels\")\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            task_ids=None\n",
    "        )\n",
    "        if torch.isnan(outputs.loss).any() or torch.isinf(outputs.loss).any():\n",
    "            print(f\"[Error] NaN/Inf loss at step {step}\")\n",
    "            print(f\"Logits: {outputs.logits}\")\n",
    "            continue\n",
    "        # Check for NaN/Inf loss\n",
    "        \n",
    "        # Decode predictions occasionally\n",
    "        if step == 0:\n",
    "            print(f\"[Pixel Stats] min: {pixel_values.min()}, max: {pixel_values.max()}, mean: {pixel_values.mean()}\")\n",
    "\n",
    "            predicted_ids = outputs.logits.argmax(dim=-1)\n",
    "            predicted_texts = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            decoded_labels = labels.clone()\n",
    "            decoded_labels[decoded_labels == -100] = processor.tokenizer.pad_token_id\n",
    "            ground_truth_texts = processor.batch_decode(decoded_labels, skip_special_tokens=True)\n",
    "\n",
    "            input_prompts = processor.batch_decode(input_ids, skip_special_tokens=True)\n",
    "            for i in range(min(2, len(predicted_texts))):\n",
    "                print(f\"\\n[Prompt]      {input_prompts[i]}\")\n",
    "                print(f\"[Prediction]  {predicted_texts[i]}\")\n",
    "                print(f\"[GroundTruth] {ground_truth_texts[i]}\")\n",
    "\n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"[Warning] NaN/Inf loss at step {step} — skipping batch\")\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # Lower clipping norm to prevent gradients explosion\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"  Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} average loss: {avg_loss:.4f}\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            input_ids = val_batch[\"input_ids\"].to(device)\n",
    "            pixel_values = val_batch[\"pixel_values\"].to(device)\n",
    "            attention_mask = val_batch[\"attention_mask\"].to(device)\n",
    "            labels = val_batch[\"labels\"].to(device)\n",
    "\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "            if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n",
    "                print(\"[Validation Error] NaN/Inf in pixel values\")\n",
    "                continue\n",
    "            if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "                print(\"[Validation Error] NaN/Inf in labels\")\n",
    "                continue\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            if torch.isnan(outputs.loss).any() or torch.isinf(outputs.loss).any():\n",
    "                print(f\"[Validation Error] NaN/Inf loss at val_step {val_step}\")\n",
    "                continue\n",
    "\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation loss after epoch {epoch + 1}: {avg_val_loss:.4f}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T21:14:08.115973Z",
     "iopub.status.busy": "2025-05-14T21:14:08.115673Z",
     "iopub.status.idle": "2025-05-14T21:14:08.134536Z",
     "shell.execute_reply": "2025-05-14T21:14:08.133711Z",
     "shell.execute_reply.started": "2025-05-14T21:14:08.115946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image_id  10000 non-null  object\n",
      " 1   path      10000 non-null  object\n",
      " 2   question  10000 non-null  object\n",
      " 3   answer    10000 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:03:22.231376Z",
     "iopub.status.busy": "2025-05-18T07:03:22.231149Z",
     "iopub.status.idle": "2025-05-18T07:03:23.795795Z",
     "shell.execute_reply": "2025-05-18T07:03:23.795095Z",
     "shell.execute_reply.started": "2025-05-18T07:03:22.231359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blip2_trained_model/processor_config.json']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model to output directory\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"blip2_trained_lora_model\")\n",
    "\n",
    "# Save processor (tokenizer + image processor)\n",
    "processor.save_pretrained(\"blip2_trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T07:03:23.796999Z",
     "iopub.status.busy": "2025-05-18T07:03:23.796599Z",
     "iopub.status.idle": "2025-05-18T07:03:53.636519Z",
     "shell.execute_reply": "2025-05-18T07:03:53.635882Z",
     "shell.execute_reply.started": "2025-05-18T07:03:23.796976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/blip2_trained_lora_model.zip'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# please download the blip2_trained_model or else all training data will be lost\n",
    "import shutil\n",
    "\n",
    "# Zip the folder\n",
    "shutil.make_archive(\"blip2_trained_lora_model\", 'zip', \"blip2_trained_lora_model\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7193652,
     "sourceId": 11547949,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7325414,
     "sourceId": 11672324,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7417410,
     "sourceId": 11810139,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7414220,
     "sourceId": 11811455,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7441188,
     "sourceId": 11843441,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7449854,
     "sourceId": 11856231,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7450483,
     "sourceId": 11857211,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
