{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11547949,"sourceType":"datasetVersion","datasetId":7193652},{"sourceId":11672324,"sourceType":"datasetVersion","datasetId":7325414},{"sourceId":11712561,"sourceType":"datasetVersion","datasetId":7351956}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# df for question answer file csv contains 1 Lakh data points\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/qna-1l/cleaned_qna.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:01:43.157095Z","iopub.execute_input":"2025-05-13T10:01:43.157379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# to read images.csv from vr-miniproject-2 or one can upload it manually and then copy the relative path\nfile_path = pd.read_csv(\"/kaggle/input/vr-miniproject-2/abo-images-small/images/metadata/images.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:04:45.002852Z","iopub.execute_input":"2025-05-13T10:04:45.003241Z","iopub.status.idle":"2025-05-13T10:04:45.631745Z","shell.execute_reply.started":"2025-05-13T10:04:45.003208Z","shell.execute_reply":"2025-05-13T10:04:45.631034Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"# merging qna with images\nfinal_df = pd.merge(df, file_path, on='image_id', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:04:47.651830Z","iopub.execute_input":"2025-05-13T10:04:47.652607Z","iopub.status.idle":"2025-05-13T10:04:47.887300Z","shell.execute_reply.started":"2025-05-13T10:04:47.652575Z","shell.execute_reply":"2025-05-13T10:04:47.886390Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"# to select specific col\nfinal_df = final_df[[\"image_id\",\"path\",\"question\",\"answer\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:04:49.114913Z","iopub.execute_input":"2025-05-13T10:04:49.115209Z","iopub.status.idle":"2025-05-13T10:04:49.124706Z","shell.execute_reply.started":"2025-05-13T10:04:49.115190Z","shell.execute_reply":"2025-05-13T10:04:49.123818Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"# adjusting the path of images according to kaggle\nimport os\nimage_folder = '/kaggle/input/vr-miniproject-2/abo-images-small/images/small/'\nfinal_df['path'] = final_df['path'].apply(lambda p: os.path.join(image_folder, p))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:04:50.404821Z","iopub.execute_input":"2025-05-13T10:04:50.405815Z","iopub.status.idle":"2025-05-13T10:04:50.507340Z","shell.execute_reply.started":"2025-05-13T10:04:50.405780Z","shell.execute_reply":"2025-05-13T10:04:50.506419Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"# install bert score package\n!pip install bert-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:04:51.786598Z","iopub.execute_input":"2025-05-13T10:04:51.786991Z","iopub.status.idle":"2025-05-13T10:04:55.910849Z","shell.execute_reply.started":"2025-05-13T10:04:51.786964Z","shell.execute_reply":"2025-05-13T10:04:55.909569Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.31.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.4.26)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\n","output_type":"stream"}],"execution_count":111},{"cell_type":"code","source":"# install bart score\n!git clone https://github.com/neulab/BARTScore.git\nimport sys\nsys.path.append('/kaggle/working/BARTScore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:04:57.538820Z","iopub.execute_input":"2025-05-13T10:04:57.539232Z","iopub.status.idle":"2025-05-13T10:04:57.716572Z","shell.execute_reply.started":"2025-05-13T10:04:57.539200Z","shell.execute_reply":"2025-05-13T10:04:57.715295Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'BARTScore' already exists and is not an empty directory.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":112},{"cell_type":"code","source":"# requiered classes to do fine tuning\n!pip install -q git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:05:00.826602Z","iopub.execute_input":"2025-05-13T10:05:00.826994Z","iopub.status.idle":"2025-05-13T10:05:12.341709Z","shell.execute_reply.started":"2025-05-13T10:05:00.826964Z","shell.execute_reply":"2025-05-13T10:05:12.340686Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"# just to run some sample to test if the model is working or not, if one is sure one can work just remove .iloc[0:1000]\ndf = final_df.iloc[0:1000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:05:14.963886Z","iopub.execute_input":"2025-05-13T10:05:14.964765Z","iopub.status.idle":"2025-05-13T10:05:14.973605Z","shell.execute_reply.started":"2025-05-13T10:05:14.964727Z","shell.execute_reply":"2025-05-13T10:05:14.972621Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"# change question and answert to lower case\ndf[\"question\"] = df[\"question\"].str.lower()\ndf[\"answer\"] = df[\"answer\"].str.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:05:16.905140Z","iopub.execute_input":"2025-05-13T10:05:16.905780Z","iopub.status.idle":"2025-05-13T10:05:16.914327Z","shell.execute_reply.started":"2025-05-13T10:05:16.905752Z","shell.execute_reply":"2025-05-13T10:05:16.913360Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/821518018.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"question\"] = df[\"question\"].str.lower()\n/tmp/ipykernel_35/821518018.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"answer\"] = df[\"answer\"].str.lower()\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"# in this you can change the batch size so that it does not take time but to note that increasing batch size too much gives some issue like memory issue\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoProcessor, ViltForQuestionAnswering, BitsAndBytesConfig, ViltProcessor\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom torchvision.transforms import Compose, Resize, ToTensor, Normalize\n\n# load model\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nmodel = ViltForQuestionAnswering.from_pretrained(\n    \"dandelin/vilt-b32-finetuned-vqa\",\n    device_map=\"auto\",  # Automatically allocates model across devices\n    low_cpu_mem_usage=True\n)\nfor name, module in model.named_modules():\n    # print(\"hello\")\n    print(name)\n# to create dataset for vqa for fine tuning\nclass VQADataset(Dataset):\n    def __init__(self, dataframe, processor, max_target_length=4):\n        self.dataframe = dataframe\n        self.processor = processor\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        question = self.dataframe.iloc[idx][\"question\"]\n        image_path = self.dataframe.iloc[idx][\"path\"]\n        answer = self.dataframe.iloc[idx][\"answer\"]\n\n    \n        image = Image.open(image_path).convert(\"RGB\")\n        prompt = f\"Answer in one word only: {question}\"\n    \n        # Step 1: Process image\n        image_encoding = self.processor.image_processor(\n            images=image,\n            return_tensors=\"pt\",\n            do_resize=True,\n            size={\"height\": 384, \"width\": 384}\n        )\n    \n        # Step 2: Process text\n        text_encoding = self.processor.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=50,\n            return_tensors=\"pt\"\n        )\n    \n        # Step 3: Combine and squeeze batch dim\n        encoding = {\n            \"input_ids\": text_encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": text_encoding[\"attention_mask\"].squeeze(0),\n            \"pixel_values\": image_encoding[\"pixel_values\"].squeeze(0),\n            \"labels\": torch.tensor(self.label2id[answer])\n        }\n    \n        return encoding\n\n\n# Collate function\ndef collate_fn(batch):\n    return {\n        key: torch.stack([item[key] for item in batch])\n        for key in batch[0]\n    }\n\n# Data split\ntrain_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\ntrain_dataset = VQADataset(train_df, processor)\nval_dataset = VQADataset(val_df, processor)\n\n# one can change batch size inorder to run it faster (if it's very large then leads to out of memory issue)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn)\n# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:24:32.629620Z","iopub.execute_input":"2025-05-13T10:24:32.630074Z","iopub.status.idle":"2025-05-13T10:24:35.077062Z","shell.execute_reply.started":"2025-05-13T10:24:32.630038Z","shell.execute_reply":"2025-05-13T10:24:35.076005Z"}},"outputs":[{"name":"stdout","text":"\nvilt\nvilt.embeddings\nvilt.embeddings.text_embeddings\nvilt.embeddings.text_embeddings.word_embeddings\nvilt.embeddings.text_embeddings.position_embeddings\nvilt.embeddings.text_embeddings.token_type_embeddings\nvilt.embeddings.text_embeddings.LayerNorm\nvilt.embeddings.text_embeddings.dropout\nvilt.embeddings.patch_embeddings\nvilt.embeddings.patch_embeddings.projection\nvilt.embeddings.token_type_embeddings\nvilt.embeddings.dropout\nvilt.encoder\nvilt.encoder.layer\nvilt.encoder.layer.0\nvilt.encoder.layer.0.attention\nvilt.encoder.layer.0.attention.attention\nvilt.encoder.layer.0.attention.attention.query\nvilt.encoder.layer.0.attention.attention.key\nvilt.encoder.layer.0.attention.attention.value\nvilt.encoder.layer.0.attention.attention.dropout\nvilt.encoder.layer.0.attention.output\nvilt.encoder.layer.0.attention.output.dense\nvilt.encoder.layer.0.attention.output.dropout\nvilt.encoder.layer.0.intermediate\nvilt.encoder.layer.0.intermediate.dense\nvilt.encoder.layer.0.intermediate.intermediate_act_fn\nvilt.encoder.layer.0.output\nvilt.encoder.layer.0.output.dense\nvilt.encoder.layer.0.output.dropout\nvilt.encoder.layer.0.layernorm_before\nvilt.encoder.layer.0.layernorm_after\nvilt.encoder.layer.1\nvilt.encoder.layer.1.attention\nvilt.encoder.layer.1.attention.attention\nvilt.encoder.layer.1.attention.attention.query\nvilt.encoder.layer.1.attention.attention.key\nvilt.encoder.layer.1.attention.attention.value\nvilt.encoder.layer.1.attention.attention.dropout\nvilt.encoder.layer.1.attention.output\nvilt.encoder.layer.1.attention.output.dense\nvilt.encoder.layer.1.attention.output.dropout\nvilt.encoder.layer.1.intermediate\nvilt.encoder.layer.1.intermediate.dense\nvilt.encoder.layer.1.intermediate.intermediate_act_fn\nvilt.encoder.layer.1.output\nvilt.encoder.layer.1.output.dense\nvilt.encoder.layer.1.output.dropout\nvilt.encoder.layer.1.layernorm_before\nvilt.encoder.layer.1.layernorm_after\nvilt.encoder.layer.2\nvilt.encoder.layer.2.attention\nvilt.encoder.layer.2.attention.attention\nvilt.encoder.layer.2.attention.attention.query\nvilt.encoder.layer.2.attention.attention.key\nvilt.encoder.layer.2.attention.attention.value\nvilt.encoder.layer.2.attention.attention.dropout\nvilt.encoder.layer.2.attention.output\nvilt.encoder.layer.2.attention.output.dense\nvilt.encoder.layer.2.attention.output.dropout\nvilt.encoder.layer.2.intermediate\nvilt.encoder.layer.2.intermediate.dense\nvilt.encoder.layer.2.intermediate.intermediate_act_fn\nvilt.encoder.layer.2.output\nvilt.encoder.layer.2.output.dense\nvilt.encoder.layer.2.output.dropout\nvilt.encoder.layer.2.layernorm_before\nvilt.encoder.layer.2.layernorm_after\nvilt.encoder.layer.3\nvilt.encoder.layer.3.attention\nvilt.encoder.layer.3.attention.attention\nvilt.encoder.layer.3.attention.attention.query\nvilt.encoder.layer.3.attention.attention.key\nvilt.encoder.layer.3.attention.attention.value\nvilt.encoder.layer.3.attention.attention.dropout\nvilt.encoder.layer.3.attention.output\nvilt.encoder.layer.3.attention.output.dense\nvilt.encoder.layer.3.attention.output.dropout\nvilt.encoder.layer.3.intermediate\nvilt.encoder.layer.3.intermediate.dense\nvilt.encoder.layer.3.intermediate.intermediate_act_fn\nvilt.encoder.layer.3.output\nvilt.encoder.layer.3.output.dense\nvilt.encoder.layer.3.output.dropout\nvilt.encoder.layer.3.layernorm_before\nvilt.encoder.layer.3.layernorm_after\nvilt.encoder.layer.4\nvilt.encoder.layer.4.attention\nvilt.encoder.layer.4.attention.attention\nvilt.encoder.layer.4.attention.attention.query\nvilt.encoder.layer.4.attention.attention.key\nvilt.encoder.layer.4.attention.attention.value\nvilt.encoder.layer.4.attention.attention.dropout\nvilt.encoder.layer.4.attention.output\nvilt.encoder.layer.4.attention.output.dense\nvilt.encoder.layer.4.attention.output.dropout\nvilt.encoder.layer.4.intermediate\nvilt.encoder.layer.4.intermediate.dense\nvilt.encoder.layer.4.intermediate.intermediate_act_fn\nvilt.encoder.layer.4.output\nvilt.encoder.layer.4.output.dense\nvilt.encoder.layer.4.output.dropout\nvilt.encoder.layer.4.layernorm_before\nvilt.encoder.layer.4.layernorm_after\nvilt.encoder.layer.5\nvilt.encoder.layer.5.attention\nvilt.encoder.layer.5.attention.attention\nvilt.encoder.layer.5.attention.attention.query\nvilt.encoder.layer.5.attention.attention.key\nvilt.encoder.layer.5.attention.attention.value\nvilt.encoder.layer.5.attention.attention.dropout\nvilt.encoder.layer.5.attention.output\nvilt.encoder.layer.5.attention.output.dense\nvilt.encoder.layer.5.attention.output.dropout\nvilt.encoder.layer.5.intermediate\nvilt.encoder.layer.5.intermediate.dense\nvilt.encoder.layer.5.intermediate.intermediate_act_fn\nvilt.encoder.layer.5.output\nvilt.encoder.layer.5.output.dense\nvilt.encoder.layer.5.output.dropout\nvilt.encoder.layer.5.layernorm_before\nvilt.encoder.layer.5.layernorm_after\nvilt.encoder.layer.6\nvilt.encoder.layer.6.attention\nvilt.encoder.layer.6.attention.attention\nvilt.encoder.layer.6.attention.attention.query\nvilt.encoder.layer.6.attention.attention.key\nvilt.encoder.layer.6.attention.attention.value\nvilt.encoder.layer.6.attention.attention.dropout\nvilt.encoder.layer.6.attention.output\nvilt.encoder.layer.6.attention.output.dense\nvilt.encoder.layer.6.attention.output.dropout\nvilt.encoder.layer.6.intermediate\nvilt.encoder.layer.6.intermediate.dense\nvilt.encoder.layer.6.intermediate.intermediate_act_fn\nvilt.encoder.layer.6.output\nvilt.encoder.layer.6.output.dense\nvilt.encoder.layer.6.output.dropout\nvilt.encoder.layer.6.layernorm_before\nvilt.encoder.layer.6.layernorm_after\nvilt.encoder.layer.7\nvilt.encoder.layer.7.attention\nvilt.encoder.layer.7.attention.attention\nvilt.encoder.layer.7.attention.attention.query\nvilt.encoder.layer.7.attention.attention.key\nvilt.encoder.layer.7.attention.attention.value\nvilt.encoder.layer.7.attention.attention.dropout\nvilt.encoder.layer.7.attention.output\nvilt.encoder.layer.7.attention.output.dense\nvilt.encoder.layer.7.attention.output.dropout\nvilt.encoder.layer.7.intermediate\nvilt.encoder.layer.7.intermediate.dense\nvilt.encoder.layer.7.intermediate.intermediate_act_fn\nvilt.encoder.layer.7.output\nvilt.encoder.layer.7.output.dense\nvilt.encoder.layer.7.output.dropout\nvilt.encoder.layer.7.layernorm_before\nvilt.encoder.layer.7.layernorm_after\nvilt.encoder.layer.8\nvilt.encoder.layer.8.attention\nvilt.encoder.layer.8.attention.attention\nvilt.encoder.layer.8.attention.attention.query\nvilt.encoder.layer.8.attention.attention.key\nvilt.encoder.layer.8.attention.attention.value\nvilt.encoder.layer.8.attention.attention.dropout\nvilt.encoder.layer.8.attention.output\nvilt.encoder.layer.8.attention.output.dense\nvilt.encoder.layer.8.attention.output.dropout\nvilt.encoder.layer.8.intermediate\nvilt.encoder.layer.8.intermediate.dense\nvilt.encoder.layer.8.intermediate.intermediate_act_fn\nvilt.encoder.layer.8.output\nvilt.encoder.layer.8.output.dense\nvilt.encoder.layer.8.output.dropout\nvilt.encoder.layer.8.layernorm_before\nvilt.encoder.layer.8.layernorm_after\nvilt.encoder.layer.9\nvilt.encoder.layer.9.attention\nvilt.encoder.layer.9.attention.attention\nvilt.encoder.layer.9.attention.attention.query\nvilt.encoder.layer.9.attention.attention.key\nvilt.encoder.layer.9.attention.attention.value\nvilt.encoder.layer.9.attention.attention.dropout\nvilt.encoder.layer.9.attention.output\nvilt.encoder.layer.9.attention.output.dense\nvilt.encoder.layer.9.attention.output.dropout\nvilt.encoder.layer.9.intermediate\nvilt.encoder.layer.9.intermediate.dense\nvilt.encoder.layer.9.intermediate.intermediate_act_fn\nvilt.encoder.layer.9.output\nvilt.encoder.layer.9.output.dense\nvilt.encoder.layer.9.output.dropout\nvilt.encoder.layer.9.layernorm_before\nvilt.encoder.layer.9.layernorm_after\nvilt.encoder.layer.10\nvilt.encoder.layer.10.attention\nvilt.encoder.layer.10.attention.attention\nvilt.encoder.layer.10.attention.attention.query\nvilt.encoder.layer.10.attention.attention.key\nvilt.encoder.layer.10.attention.attention.value\nvilt.encoder.layer.10.attention.attention.dropout\nvilt.encoder.layer.10.attention.output\nvilt.encoder.layer.10.attention.output.dense\nvilt.encoder.layer.10.attention.output.dropout\nvilt.encoder.layer.10.intermediate\nvilt.encoder.layer.10.intermediate.dense\nvilt.encoder.layer.10.intermediate.intermediate_act_fn\nvilt.encoder.layer.10.output\nvilt.encoder.layer.10.output.dense\nvilt.encoder.layer.10.output.dropout\nvilt.encoder.layer.10.layernorm_before\nvilt.encoder.layer.10.layernorm_after\nvilt.encoder.layer.11\nvilt.encoder.layer.11.attention\nvilt.encoder.layer.11.attention.attention\nvilt.encoder.layer.11.attention.attention.query\nvilt.encoder.layer.11.attention.attention.key\nvilt.encoder.layer.11.attention.attention.value\nvilt.encoder.layer.11.attention.attention.dropout\nvilt.encoder.layer.11.attention.output\nvilt.encoder.layer.11.attention.output.dense\nvilt.encoder.layer.11.attention.output.dropout\nvilt.encoder.layer.11.intermediate\nvilt.encoder.layer.11.intermediate.dense\nvilt.encoder.layer.11.intermediate.intermediate_act_fn\nvilt.encoder.layer.11.output\nvilt.encoder.layer.11.output.dense\nvilt.encoder.layer.11.output.dropout\nvilt.encoder.layer.11.layernorm_before\nvilt.encoder.layer.11.layernorm_after\nvilt.layernorm\nvilt.pooler\nvilt.pooler.dense\nvilt.pooler.activation\nclassifier\nclassifier.0\nclassifier.1\nclassifier.2\nclassifier.3\n","output_type":"stream"}],"execution_count":138},{"cell_type":"code","source":"# this is used to fund module names which can be used for fine tuning as it should be all linear\nfor name, module in model.named_modules():\n    if isinstance(module, torch.nn.Linear):\n        print(name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:04:52.387153Z","iopub.execute_input":"2025-05-13T09:04:52.387480Z","iopub.status.idle":"2025-05-13T09:04:52.393915Z","shell.execute_reply.started":"2025-05-13T09:04:52.387458Z","shell.execute_reply":"2025-05-13T09:04:52.392960Z"}},"outputs":[{"name":"stdout","text":"vilt.encoder.layer.0.attention.attention.query\nvilt.encoder.layer.0.attention.attention.key\nvilt.encoder.layer.0.attention.attention.value\nvilt.encoder.layer.0.attention.output.dense\nvilt.encoder.layer.0.intermediate.dense\nvilt.encoder.layer.0.output.dense\nvilt.encoder.layer.1.attention.attention.query\nvilt.encoder.layer.1.attention.attention.key\nvilt.encoder.layer.1.attention.attention.value\nvilt.encoder.layer.1.attention.output.dense\nvilt.encoder.layer.1.intermediate.dense\nvilt.encoder.layer.1.output.dense\nvilt.encoder.layer.2.attention.attention.query\nvilt.encoder.layer.2.attention.attention.key\nvilt.encoder.layer.2.attention.attention.value\nvilt.encoder.layer.2.attention.output.dense\nvilt.encoder.layer.2.intermediate.dense\nvilt.encoder.layer.2.output.dense\nvilt.encoder.layer.3.attention.attention.query\nvilt.encoder.layer.3.attention.attention.key\nvilt.encoder.layer.3.attention.attention.value\nvilt.encoder.layer.3.attention.output.dense\nvilt.encoder.layer.3.intermediate.dense\nvilt.encoder.layer.3.output.dense\nvilt.encoder.layer.4.attention.attention.query\nvilt.encoder.layer.4.attention.attention.key\nvilt.encoder.layer.4.attention.attention.value\nvilt.encoder.layer.4.attention.output.dense\nvilt.encoder.layer.4.intermediate.dense\nvilt.encoder.layer.4.output.dense\nvilt.encoder.layer.5.attention.attention.query\nvilt.encoder.layer.5.attention.attention.key\nvilt.encoder.layer.5.attention.attention.value\nvilt.encoder.layer.5.attention.output.dense\nvilt.encoder.layer.5.intermediate.dense\nvilt.encoder.layer.5.output.dense\nvilt.encoder.layer.6.attention.attention.query\nvilt.encoder.layer.6.attention.attention.key\nvilt.encoder.layer.6.attention.attention.value\nvilt.encoder.layer.6.attention.output.dense\nvilt.encoder.layer.6.intermediate.dense\nvilt.encoder.layer.6.output.dense\nvilt.encoder.layer.7.attention.attention.query\nvilt.encoder.layer.7.attention.attention.key\nvilt.encoder.layer.7.attention.attention.value\nvilt.encoder.layer.7.attention.output.dense\nvilt.encoder.layer.7.intermediate.dense\nvilt.encoder.layer.7.output.dense\nvilt.encoder.layer.8.attention.attention.query\nvilt.encoder.layer.8.attention.attention.key\nvilt.encoder.layer.8.attention.attention.value\nvilt.encoder.layer.8.attention.output.dense\nvilt.encoder.layer.8.intermediate.dense\nvilt.encoder.layer.8.output.dense\nvilt.encoder.layer.9.attention.attention.query\nvilt.encoder.layer.9.attention.attention.key\nvilt.encoder.layer.9.attention.attention.value\nvilt.encoder.layer.9.attention.output.dense\nvilt.encoder.layer.9.intermediate.dense\nvilt.encoder.layer.9.output.dense\nvilt.encoder.layer.10.attention.attention.query\nvilt.encoder.layer.10.attention.attention.key\nvilt.encoder.layer.10.attention.attention.value\nvilt.encoder.layer.10.attention.output.dense\nvilt.encoder.layer.10.intermediate.dense\nvilt.encoder.layer.10.output.dense\nvilt.encoder.layer.11.attention.attention.query\nvilt.encoder.layer.11.attention.attention.key\nvilt.encoder.layer.11.attention.attention.value\nvilt.encoder.layer.11.attention.output.dense\nvilt.encoder.layer.11.intermediate.dense\nvilt.encoder.layer.11.output.dense\nvilt.pooler.dense\nclassifier.0\nclassifier.3\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"# PEFT: LoRA configuration \n# one can change here r, alpha, dropout, bias, target_modules (for target_modules select from above layers if u want additional modules)\n\n# lora_config = LoraConfig(\n#     r=128,\n#     lora_alpha=128,\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     target_modules=[\"attention.self.query\", \"attention.self.value\"]\n# )\npeft_config = LoraConfig(\n    \n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\",\n    target_modules=[\"query\", \"key\", \"value\"]\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:24:42.907746Z","iopub.execute_input":"2025-05-13T10:24:42.908349Z","iopub.status.idle":"2025-05-13T10:24:43.012089Z","shell.execute_reply.started":"2025-05-13T10:24:42.908321Z","shell.execute_reply":"2025-05-13T10:24:43.010940Z"}},"outputs":[{"name":"stdout","text":"trainable params: 442,368 || all params: 118,030,905 || trainable%: 0.3748\n","output_type":"stream"}],"execution_count":139},{"cell_type":"code","source":"print(df.columns)  # or whatever your DataFrame is named","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:23:39.115675Z","iopub.execute_input":"2025-05-13T10:23:39.116055Z","iopub.status.idle":"2025-05-13T10:23:39.122424Z","shell.execute_reply.started":"2025-05-13T10:23:39.116030Z","shell.execute_reply":"2025-05-13T10:23:39.121378Z"}},"outputs":[{"name":"stdout","text":"Index(['image_id', 'path', 'question', 'answer'], dtype='object')\n","output_type":"stream"}],"execution_count":137},{"cell_type":"code","source":"# was trained some more time like 30 iteratins\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-6)  # Slightly increased learning rate\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\nmodel = model.to(torch.float32)\nmodel.train()\n\n# can change number of iteration  here\nfor epoch in range(20): \n    print(f\"\\nEpoch {epoch + 1}\")\n    total_loss = 0\n\n    for step, batch in enumerate(train_loader):\n        input_ids = batch[\"input_ids\"].to(device)\n        pixel_values = batch[\"pixel_values\"].to(device)  # defaults to float32\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Mask padding tokens in labels to avoid loss explosion\n        labels[labels == processor.tokenizer.pad_token_id] = -100\n\n        # Check for NaN/Inf in pixel values and labels before forward pass\n        if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n            print(\"[Error] NaN/Inf in pixel values\")\n            continue\n        if torch.isnan(labels).any() or torch.isinf(labels).any():\n            print(\"[Error] NaN/Inf in labels\")\n            continue\n\n        # Forward pass\n        outputs = model(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        if torch.isnan(outputs.loss).any() or torch.isinf(outputs.loss).any():\n            print(f\"[Error] NaN/Inf loss at step {step}\")\n            print(f\"Logits: {outputs.logits}\")\n            continue\n        # Check for NaN/Inf loss\n        \n        # Decode predictions occasionally\n        if step == 0:\n            print(f\"[Pixel Stats] min: {pixel_values.min()}, max: {pixel_values.max()}, mean: {pixel_values.mean()}\")\n\n            predicted_ids = outputs.logits.argmax(dim=-1)\n            predicted_texts = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n            decoded_labels = labels.clone()\n            decoded_labels[decoded_labels == -100] = processor.tokenizer.pad_token_id\n            ground_truth_texts = processor.batch_decode(decoded_labels, skip_special_tokens=True)\n\n            input_prompts = processor.batch_decode(input_ids, skip_special_tokens=True)\n            for i in range(min(2, len(predicted_texts))):\n                print(f\"\\n[Prompt]      {input_prompts[i]}\")\n                print(f\"[Prediction]  {predicted_texts[i]}\")\n                print(f\"[GroundTruth] {ground_truth_texts[i]}\")\n\n        # Compute loss\n        loss = outputs.loss\n\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"[Warning] NaN/Inf loss at step {step} — skipping batch\")\n            continue\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # Lower clipping norm to prevent gradients explosion\n        optimizer.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n        if step % 10 == 0:\n            print(f\"  Step {step}: Loss = {loss.item():.4f}\")\n\n    # Print average loss for the epoch\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1} average loss: {avg_loss:.4f}\")\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for val_step, val_batch in enumerate(val_loader):\n            input_ids = val_batch[\"input_ids\"].to(device)\n            pixel_values = val_batch[\"pixel_values\"].to(device)\n            attention_mask = val_batch[\"attention_mask\"].to(device)\n            labels = val_batch[\"labels\"].to(device)\n\n            labels[labels == processor.tokenizer.pad_token_id] = -100\n\n            if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n                print(\"[Validation Error] NaN/Inf in pixel values\")\n                continue\n            if torch.isnan(labels).any() or torch.isinf(labels).any():\n                print(\"[Validation Error] NaN/Inf in labels\")\n                continue\n\n            outputs = model(\n                input_ids=input_ids,\n                pixel_values=pixel_values,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n            if torch.isnan(outputs.loss).any() or torch.isinf(outputs.loss).any():\n                print(f\"[Validation Error] NaN/Inf loss at val_step {val_step}\")\n                continue\n\n            val_loss += outputs.loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Validation loss after epoch {epoch + 1}: {avg_val_loss:.4f}\")\n    model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:24:47.663095Z","iopub.execute_input":"2025-05-13T10:24:47.663435Z","iopub.status.idle":"2025-05-13T10:24:47.811748Z","shell.execute_reply.started":"2025-05-13T10:24:47.663398Z","shell.execute_reply":"2025-05-13T10:24:47.809930Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1802551548.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# defaults to float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2670153184.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Step 1: Process image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         image_encoding = self.processor.image_processor(\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvalid_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vilt/image_processing_vilt.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, images, do_resize, size, size_divisor, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_pad, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             images = [\n\u001b[0m\u001b[1;32m    453\u001b[0m                 self.resize(\n\u001b[1;32m    454\u001b[0m                     \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vilt/image_processing_vilt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             images = [\n\u001b[0;32m--> 453\u001b[0;31m                 self.resize(\n\u001b[0m\u001b[1;32m    454\u001b[0m                     \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vilt/image_processing_vilt.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, size, size_divisor, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_size_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_to_square\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"shortest_edge\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The `size` dictionary must contain the key `shortest_edge`. Got {size.keys()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0mshorter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shortest_edge\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mlonger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1333\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m800\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshorter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The `size` dictionary must contain the key `shortest_edge`. Got dict_keys(['height', 'width'])"],"ename":"ValueError","evalue":"The `size` dictionary must contain the key `shortest_edge`. Got dict_keys(['height', 'width'])","output_type":"error"}],"execution_count":140},{"cell_type":"code","source":"# save the model to output directory\nfrom transformers import AutoProcessor\n\n# Save model\nmodel.save_pretrained(\"blip2_trained_lora_model\")\n\n# Save processor (tokenizer + image processor)\nprocessor.save_pretrained(\"blip2_trained_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T06:43:16.850210Z","iopub.execute_input":"2025-05-11T06:43:16.850424Z","iopub.status.idle":"2025-05-11T06:43:17.835647Z","shell.execute_reply.started":"2025-05-11T06:43:16.850408Z","shell.execute_reply":"2025-05-11T06:43:17.835089Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['blip2_trained_model/processor_config.json']"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# please download the blip2_trained_model or else all training data will be lost\nimport shutil\n\n# Zip the folder\nshutil.make_archive(\"blip2_trained_lora_model\", 'zip', \"blip2_trained_lora_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T06:43:17.836275Z","iopub.execute_input":"2025-05-11T06:43:17.836501Z","iopub.status.idle":"2025-05-11T06:43:33.086911Z","shell.execute_reply.started":"2025-05-11T06:43:17.836476Z","shell.execute_reply":"2025-05-11T06:43:33.085981Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/blip2_trained_lora_model.zip'"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# main blip code to generate words for prediction with taking last word\n# for index, row in final_df.head(5).iterrows(): \n# in the above line change the number of samples of like 5 to 10 and wish to run for whole then just put: for index, row in final_df.iterrows(): \n# and if one does not want to see prediction and all comment down the print commands in for loop\nimport os\nimport torch\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration, logging\nfrom bert_score import score as bertscore\nfrom bart_score import BARTScorer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom transformers import Blip2ForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\n\n# the warning which u will see below is because we are not training all the target modules so lora model does not have all the values so it gaves warning that these modeules are missing\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Found missing adapter keys while loading the checkpoint\")\n\n# Suppress logs\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\nlogging.set_verbosity_error()\n\n# if using downloaded lora weights change the finetuned_path to the path of uploaded model\nfinetuned_path = \"/kaggle/working/blip2_trained_lora_model\"\nfrom transformers import BitsAndBytesConfig\n\nquant_config = BitsAndBytesConfig(load_in_8bit=True)\n\nbase_model = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-flan-t5-xl\",\n    device_map=\"auto\",\n    quantization_config=quant_config,\n)\n\nmodel = PeftModel.from_pretrained(base_model, finetuned_path)\n\n# Load processor\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n\nmodel.eval()\n\n\nbart_scorer = BARTScorer(device=\"cuda\" if torch.cuda.is_available() else \"cpu\", checkpoint='facebook/bart-large-cnn')\n\n# Stats\ntrue_labels = []\npred_labels = []\nbert_P_all = []\nbert_R_all = []\nbert_F1_all = []\nbart_scores_all = []\n\n# Loop through samples (remove `.head(n)` to run full)\nfor index, row in final_df.iloc[20000:20040].iterrows():\n    try:\n        question = row[\"question\"]\n        true_ans = row[\"answer\"].strip().lower()\n        image_path = row[\"path\"]\n\n        image = Image.open(image_path).convert(\"RGB\")\n\n        prompt = f\"Answer in one word only: {question}\"\n        inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n        generated_ids = model.generate(**inputs, max_new_tokens=4)\n        pred_ans = processor.decode(generated_ids[0], skip_special_tokens=True).strip().lower()\n        # pred_ans = pred_ans.split()[-1]\n        \n\n        true_labels.append(true_ans)\n        pred_labels.append(pred_ans)\n\n        # BERTScore\n        P, R, F1 = bertscore([pred_ans], [true_ans], lang=\"en\", rescale_with_baseline=True)\n        bart = bart_scorer.score([pred_ans], [true_ans])[0]\n\n        bert_P_all.append(P[0].item())\n        bert_R_all.append(R[0].item())\n        bert_F1_all.append(F1[0].item())\n        bart_scores_all.append(bart)\n\n        print(f\"\\nRow {index} | Image: {row['image_id']}\")\n        print(f\"Q: {question}\\nPredicted: {pred_ans} | Actual: {true_ans}\")\n        print(f\"Exact Match: {pred_ans == true_ans}\")\n        print(f\"BERTScore - P: {P[0].item():.4f} | R: {R[0].item():.4f} | F1: {F1[0].item():.4f} | BARTScore: {bart:.4f}\")\n\n    except Exception as e:\n        print(f\"Skipping row {index} due to error: {e}\")\n        continue\n\n# Classification Metrics (Exact Match)\naccuracy = accuracy_score(true_labels, pred_labels)\nprecision = precision_score(true_labels, pred_labels, average='macro', zero_division=0)\nrecall = recall_score(true_labels, pred_labels, average='macro', zero_division=0)\nf1 = f1_score(true_labels, pred_labels, average='macro', zero_division=0)\n\n# Semantic Metrics\navg_bert_P = sum(bert_P_all) / len(bert_P_all) if bert_P_all else 0\navg_bert_R = sum(bert_R_all) / len(bert_R_all) if bert_R_all else 0\navg_bert_F1 = sum(bert_F1_all) / len(bert_F1_all) if bert_F1_all else 0\navg_bart = sum(bart_scores_all) / len(bart_scores_all) if bart_scores_all else 0\n\n# Final summary\nprint(\"\\n===== FINAL SUMMARY =====\")\nprint(f\"Classification Accuracy:       {accuracy * 100:.2f}%\")\nprint(f\"Precision (macro avg):         {precision * 100:.2f}%\")\nprint(f\"Recall (macro avg):            {recall * 100:.2f}%\")\nprint(f\"F1 Score (macro avg):          {f1 * 100:.2f}%\")\n\nprint(f\"\\nAvg BERTScore - P:             {avg_bert_P * 100:.2f}%\")\nprint(f\"Avg BERTScore - R:             {avg_bert_R * 100:.2f}%\")\nprint(f\"Avg BERTScore - F1:            {avg_bert_F1 * 100:.2f}%\")\nprint(f\"Avg BARTScore:                 {avg_bart:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# this was done to check for df not used for training\ntrue_labels = []\npred_labels = []\nbert_P_all = []\nbert_R_all = []\nbert_F1_all = []\nbart_scores_all = []\n\n# Loop through samples (remove `.head(n)` to run full)\nfor index, row in final_df.iloc[10001:10041].iterrows():\n    try:\n        question = row[\"question\"]\n        true_ans = row[\"answer\"].strip().lower()\n        image_path = row[\"path\"]\n\n        image = Image.open(image_path).convert(\"RGB\")\n\n        prompt = f\"Answer in one word only: {question}\"\n        inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n        generated_ids = model.generate(**inputs, max_new_tokens=4)\n        pred_ans = processor.decode(generated_ids[0], skip_special_tokens=True).strip().lower()\n        # pred_ans = pred_ans.split()[-1]\n        \n\n        true_labels.append(true_ans)\n        pred_labels.append(pred_ans)\n\n        # BERTScore\n        P, R, F1 = bertscore([pred_ans], [true_ans], lang=\"en\", rescale_with_baseline=True)\n        bart = bart_scorer.score([pred_ans], [true_ans])[0]\n\n        bert_P_all.append(P[0].item())\n        bert_R_all.append(R[0].item())\n        bert_F1_all.append(F1[0].item())\n        bart_scores_all.append(bart)\n\n        print(f\"\\nRow {index} | Image: {row['image_id']}\")\n        print(f\"Q: {question}\\nPredicted: {pred_ans} | Actual: {true_ans}\")\n        print(f\"Exact Match: {pred_ans == true_ans}\")\n        print(f\"BERTScore - P: {P[0].item():.4f} | R: {R[0].item():.4f} | F1: {F1[0].item():.4f} | BARTScore: {bart:.4f}\")\n\n    except Exception as e:\n        print(f\"Skipping row {index} due to error: {e}\")\n        continue\n\n# Classification Metrics (Exact Match)\naccuracy = accuracy_score(true_labels, pred_labels)\nprecision = precision_score(true_labels, pred_labels, average='macro', zero_division=0)\nrecall = recall_score(true_labels, pred_labels, average='macro', zero_division=0)\nf1 = f1_score(true_labels, pred_labels, average='macro', zero_division=0)\n\n# Semantic Metrics\navg_bert_P = sum(bert_P_all) / len(bert_P_all) if bert_P_all else 0\navg_bert_R = sum(bert_R_all) / len(bert_R_all) if bert_R_all else 0\navg_bert_F1 = sum(bert_F1_all) / len(bert_F1_all) if bert_F1_all else 0\navg_bart = sum(bart_scores_all) / len(bart_scores_all) if bart_scores_all else 0\n\n# Final summary\nprint(\"\\n===== FINAL SUMMARY =====\")\nprint(f\"Classification Accuracy:       {accuracy * 100:.2f}%\")\nprint(f\"Precision (macro avg):         {precision * 100:.2f}%\")\nprint(f\"Recall (macro avg):            {recall * 100:.2f}%\")\nprint(f\"F1 Score (macro avg):          {f1 * 100:.2f}%\")\n\nprint(f\"\\nAvg BERTScore - P:             {avg_bert_P * 100:.2f}%\")\nprint(f\"Avg BERTScore - R:             {avg_bert_R * 100:.2f}%\")\nprint(f\"Avg BERTScore - F1:            {avg_bert_F1 * 100:.2f}%\")\nprint(f\"Avg BARTScore:                 {avg_bart:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T06:46:01.051230Z","iopub.execute_input":"2025-05-11T06:46:01.051772Z","iopub.status.idle":"2025-05-11T06:47:00.199198Z","shell.execute_reply.started":"2025-05-11T06:46:01.051748Z","shell.execute_reply":"2025-05-11T06:47:00.198388Z"}},"outputs":[{"name":"stdout","text":"\nRow 10001 | Image: 316E26-HJUL\nQ: What is the product?\nPredicted: bag | Actual: pulses\nExact Match: False\nBERTScore - P: 0.9948 | R: 0.9948 | F1: 0.9948 | BARTScore: -5.9466\n\nRow 10002 | Image: 316GBAhhv9L\nQ: What is the bottle's top?\nPredicted: lid | Actual: white\nExact Match: False\nBERTScore - P: 0.9884 | R: 0.9884 | F1: 0.9884 | BARTScore: -7.3317\n\nRow 10003 | Image: 316GBAhhv9L\nQ: What is the liquid's hue?\nPredicted: green | Actual: yellow\nExact Match: False\nBERTScore - P: 0.8104 | R: 0.8104 | F1: 0.8107 | BARTScore: -6.0827\n\nRow 10004 | Image: 316GBAhhv9L\nQ: What is the container's form?\nPredicted: cylindrical | Actual: bottle\nExact Match: False\nBERTScore - P: -0.6180 | R: 0.4482 | F1: -0.1416 | BARTScore: -6.3076\n\nRow 10005 | Image: 316IM3U2JQL\nQ: What garment is displayed?\nPredicted: jacket | Actual: jacket\nExact Match: True\nBERTScore - P: 1.0000 | R: 1.0000 | F1: 1.0000 | BARTScore: -1.8321\n\nRow 10006 | Image: 316IM3U2JQL\nQ: What color is the coat?\nPredicted: black | Actual: black\nExact Match: True\nBERTScore - P: 1.0000 | R: 1.0000 | F1: 1.0000 | BARTScore: -2.7174\n\nRow 10007 | Image: 316IM3U2JQL\nQ: Does the coat have a hood?\nPredicted: yes | Actual: hood\nExact Match: False\nBERTScore - P: 0.8791 | R: 0.8791 | F1: 0.8793 | BARTScore: -7.8915\n\nRow 10008 | Image: 316IM3U2JQL\nQ: What is the jacket's style?\nPredicted: hooded | Actual: sporty\nExact Match: False\nBERTScore - P: 0.0766 | R: -0.0671 | F1: 0.0053 | BARTScore: -5.5012\n\nRow 10009 | Image: 316IM3U2JQL\nQ: Is the jacket zipped?\nPredicted: yes | Actual: zipped\nExact Match: False\nBERTScore - P: 0.0959 | R: -0.3865 | F1: -0.1555 | BARTScore: -7.5835\n\nRow 10010 | Image: 316IM3U2JQL\nQ: Where is the pocket?\nPredicted: hood | Actual: chest\nExact Match: False\nBERTScore - P: 0.8767 | R: 0.8767 | F1: 0.8769 | BARTScore: -6.9065\n\nRow 10011 | Image: 316IM3U2JQL\nQ: What surrounds the pocket?\nPredicted: hood | Actual: border\nExact Match: False\nBERTScore - P: 0.8801 | R: 0.8801 | F1: 0.8803 | BARTScore: -7.6601\n\nRow 10012 | Image: 316JLWjhn7L\nQ: What shape is the bottle?\nPredicted: cylindrical | Actual: cylindrical\nExact Match: True\nBERTScore - P: 1.0000 | R: 1.0000 | F1: 1.0000 | BARTScore: -1.0164\n\nRow 10013 | Image: 316JLWjhn7L\nQ: What is the bottle's color?\nPredicted: black | Actual: brown\nExact Match: False\nBERTScore - P: 0.9927 | R: 0.9927 | F1: 0.9927 | BARTScore: -6.4809\n\nRow 10014 | Image: 316JLWjhn7L\nQ: What is on the bottle?\nPredicted: label | Actual: label\nExact Match: True\nBERTScore - P: 1.0000 | R: 1.0000 | F1: 1.0000 | BARTScore: -1.7253\n\nRow 10015 | Image: 316JLWjhn7L\nQ: What covers the bottle's top?\nPredicted: lid | Actual: cap\nExact Match: False\nBERTScore - P: -0.0998 | R: -0.1958 | F1: -0.1464 | BARTScore: -7.4368\n\nRow 10016 | Image: 316JLWjhn7L\nQ: What material is the bottle?\nPredicted: glass | Actual: glass\nExact Match: True\nBERTScore - P: 1.0000 | R: 1.0000 | F1: 1.0000 | BARTScore: -2.1388\n\nRow 10017 | Image: 316L7ZiWBaL\nQ: What covers the top?\nPredicted: bag | Actual: black\nExact Match: False\nBERTScore - P: 0.9965 | R: 0.9965 | F1: 0.9966 | BARTScore: -6.7995\n\nRow 10018 | Image: 316L7ZiWBaL\nQ: What is the container's state?\nPredicted: open | Actual: sealed\nExact Match: False\nBERTScore - P: 0.9883 | R: 0.9883 | F1: 0.9883 | BARTScore: -7.2034\n\nRow 10019 | Image: 316L7ZiWBaL\nQ: What kind of food?\nPredicted: cereal | Actual: nuts\nExact Match: False\nBERTScore - P: 0.9981 | R: 0.9981 | F1: 0.9981 | BARTScore: -8.0274\n\nRow 10020 | Image: 316LnG9NULL\nQ: What is its material?\nPredicted: glass | Actual: glass\nExact Match: True\nBERTScore - P: 1.0000 | R: 1.0000 | F1: 1.0000 | BARTScore: -2.1388\n\nRow 10021 | Image: 316LnG9NULL\nQ: What is the glass color?\nPredicted: white | Actual: clear\nExact Match: False\nBERTScore - P: 0.9913 | R: 0.9913 | F1: 0.9913 | BARTScore: -7.2003\n\nRow 10022 | Image: 316LnG9NULL\nQ: What holds the liquid?\nPredicted: glass | Actual: bowl\nExact Match: False\nBERTScore - P: 0.9784 | R: 0.9784 | F1: 0.9784 | BARTScore: -6.7144\n\nRow 10023 | Image: 316LnG9NULL\nQ: What supports the bowl?\nPredicted: hand | Actual: stem\nExact Match: False\nBERTScore - P: 0.9938 | R: 0.9938 | F1: 0.9938 | BARTScore: -7.0572\n\nRow 10024 | Image: 316LnG9NULL\nQ: What is the glass' purpose?\nPredicted: wine | Actual: drinking\nExact Match: False\nBERTScore - P: 0.9962 | R: 0.9962 | F1: 0.9962 | BARTScore: -6.2197\n\nRow 10025 | Image: 316LnG9NULL\nQ: What is the glass' type?\nPredicted: glass | Actual: wine\nExact Match: False\nBERTScore - P: 0.9936 | R: 0.9936 | F1: 0.9936 | BARTScore: -6.6889\n\nRow 10026 | Image: 316O0EedsiL\nQ: What is the tub color?\nPredicted: green | Actual: white\nExact Match: False\nBERTScore - P: 0.7799 | R: 0.7799 | F1: 0.7802 | BARTScore: -6.4897\n\nRow 10027 | Image: 316O0EedsiL\nQ: What is the wrapper color?\nPredicted: green | Actual: green\nExact Match: True\nBERTScore - P: 1.0000 | R: 1.0000 | F1: 1.0000 | BARTScore: -2.5917\n\nRow 10028 | Image: 316O0EedsiL\nQ: What shape is the container?\nPredicted: cylindrical | Actual: round\nExact Match: False\nBERTScore - P: -0.6144 | R: 0.4585 | F1: -0.1353 | BARTScore: -6.6675\n\nRow 10029 | Image: 316O0EedsiL\nQ: What is the tub material?\nPredicted: plastic | Actual: plastic\nExact Match: True\nBERTScore - P: 1.0000 | R: 1.0000 | F1: 1.0000 | BARTScore: -1.6487\n\nRow 10030 | Image: 316O0EedsiL\nQ: What holds the product?\nPredicted: hand | Actual: container\nExact Match: False\nBERTScore - P: 0.9953 | R: 0.9953 | F1: 0.9953 | BARTScore: -7.6117\n\nRow 10031 | Image: 316O537ZHvL\nQ: What is yellow?\nPredicted: striped | Actual: corn\nExact Match: False\nBERTScore - P: 0.9957 | R: 0.9957 | F1: 0.9957 | BARTScore: -7.2814\n\nRow 10032 | Image: 316O537ZHvL\nQ: What color is corn?\nPredicted: white | Actual: yellow\nExact Match: False\nBERTScore - P: 0.9316 | R: 0.9316 | F1: 0.9317 | BARTScore: -6.0363\n\nRow 10033 | Image: 316O537ZHvL\nQ: What product contains soy?\nPredicted: vegetable | Actual: corn\nExact Match: False\nBERTScore - P: 0.9948 | R: 0.9948 | F1: 0.9948 | BARTScore: -6.6931\n\nRow 10034 | Image: 316O537ZHvL\nQ: What word is repeated?\nPredicted: zigza | Actual: sweet\nExact Match: False\nBERTScore - P: -0.4927 | R: 0.4310 | F1: -0.0726 | BARTScore: -8.4442\n\nRow 10035 | Image: 316O537ZHvL\nQ: What process does this facility do?\nPredicted: printing | Actual: processes\nExact Match: False\nBERTScore - P: 0.9717 | R: 0.9717 | F1: 0.9718 | BARTScore: -5.4093\n\nRow 10036 | Image: 316O537ZHvL\nQ: What ingredients are here?\nPredicted: striped | Actual: corn\nExact Match: False\nBERTScore - P: 0.9957 | R: 0.9957 | F1: 0.9957 | BARTScore: -7.2814\n\nRow 10037 | Image: 316O537ZHvL\nQ: Where is soy processed?\nPredicted: in | Actual: facility\nExact Match: False\nBERTScore - P: 0.9905 | R: 0.9905 | F1: 0.9905 | BARTScore: -6.1049\n\nRow 10038 | Image: 316OUzQ7ZOL\nQ: What color is packaging?\nPredicted: blue | Actual: pink\nExact Match: False\nBERTScore - P: 0.7350 | R: 0.7351 | F1: 0.7355 | BARTScore: -5.3446\n\nRow 10039 | Image: 316OUzQ7ZOL\nQ: What shape is package?\nPredicted: rectangular | Actual: square\nExact Match: False\nBERTScore - P: 0.9966 | R: 0.9966 | F1: 0.9966 | BARTScore: -6.4855\n\nRow 10040 | Image: 316OUzQ7ZOL\nQ: What material is box?\nPredicted: cardboard | Actual: cardboard\nExact Match: True\nBERTScore - P: 1.0000 | R: 1.0000 | F1: 1.0000 | BARTScore: -1.7624\n\n===== FINAL SUMMARY =====\nClassification Accuracy:       22.50%\nPrecision (macro avg):         11.81%\nRecall (macro avg):            15.62%\nF1 Score (macro avg):          12.85%\n\nAvg BERTScore - P:             77.73%\nAvg BERTScore - R:             83.58%\nAvg BERTScore - F1:            80.25%\nAvg BARTScore:                 -5.7115\n","output_type":"stream"}],"execution_count":24}]}